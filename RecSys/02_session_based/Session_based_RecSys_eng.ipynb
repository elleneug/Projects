{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any\n",
    "from gensim.models import Word2Vec\n",
    "import optuna\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3314a",
   "metadata": {},
   "source": [
    "## Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e55759",
   "metadata": {},
   "source": [
    "As an ML engineer in a music streaming service, we are tasked with creating a recommendation system for our own streaming service, similar to Spotify. We do not have knowledge about the content, but we have the listening history of artists for each user.\n",
    "\n",
    "Our task is to improve the algorithm that will determine the most relevant recommendations for each user based on their listening history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4d803",
   "metadata": {},
   "source": [
    "#### Data Description\n",
    "The train.parquet file provides us with data about users listening to artists on the service.\n",
    "\n",
    "| Field     | Type | Description               |\n",
    "|-----------|------|---------------------------|\n",
    "| user_id   | str  | User ID                   |\n",
    "| artist_id | str  | Artist ID               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91208510",
   "metadata": {},
   "source": [
    "### Quality Metrics\n",
    "We will use the ndcg@20 metric, which is often applied in ranking tasks. The more relevant objects are at the beginning of the recommendation list, the higher it is. Session validation with the last N artists from the listening history is used for evaluation.\n",
    "\n",
    "The code for calculating ndcg is as follows:\n",
    "\n",
    "```python\n",
    "def user_ndcg(y_rel: List[Any], y_rec: List[Any], k: int = 20) -> float:\n",
    "    \"\"\"\n",
    "    :param y_rel: relevant items\n",
    "    :param y_rec: recommended items\n",
    "    :param k: number of top recommended items\n",
    "    :return: ndcg metric for user recommendations\n",
    "    \"\"\"\n",
    "    dcg = sum([1. / np.log2(idx + 2) for idx, item in enumerate(y_rec[:k]) if item in y_rel])\n",
    "    idcg = sum([1. / np.log2(idx + 2) for idx, _ in enumerate(zip(y_rel, np.arange(k)))])\n",
    "    return dcg / idcg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86f3a9",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0fc0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pl.read_parquet('train_session_based.parquet')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0187017",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Our task will be the optimization of the ndcg@20 metric. Nevertheless, such a metric is difficult to interpret, so we will also have access to the hitrate@20 metric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 20\n",
    "\n",
    "\n",
    "def user_hitrate(y_relevant: List[str], y_recs: List[str], k: int = TOP_K) -> int:\n",
    "    return int(len(set(y_relevant).intersection(y_recs[:k])) > 0)\n",
    "\n",
    "def user_ndcg(y_rel: List[Any], y_rec: List[Any], k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    :param y_rel: relevant items\n",
    "    :param y_rec: recommended items\n",
    "    :param k: number of top recommended items\n",
    "    :return: ndcg metric for user recommendations\n",
    "    \"\"\"\n",
    "    dcg = sum([1. / np.log2(idx + 2) for idx, item in enumerate(y_rec[:k]) if item in y_rel])\n",
    "    idcg = sum([1. / np.log2(idx + 2) for idx, _ in enumerate(zip(y_rel, np.arange(k)))])\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf0329",
   "metadata": {},
   "source": [
    "In this dataset, identifiers are presented as strings, but for working with them, it might be easier to convert them into numbers (for instance, for matrix factorization algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07829521",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = {k: v for v, k in enumerate(data['user_id'].unique())}\n",
    "user_mapping_inverse = {k: v for v, k in user_mapping.items()}\n",
    "\n",
    "artist_mapping = {k: v for v, k in enumerate(data['artist_id'].unique())}\n",
    "artist_mapping_inverse = {k: v for v, k in artist_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ede3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_df_with_inds = (\n",
    "    data\n",
    "    .with_columns([\n",
    "        pl.col('user_id').apply(user_mapping.get),\n",
    "        pl.col('artist_id').apply(artist_mapping.get),\n",
    "    ])\n",
    "    # для каждого пользователя оставим последние 3 объекта в качестве тестовой выборки,\n",
    "    # а остальное будем использовать для тренировки\n",
    "    .groupby('user_id')\n",
    "    .agg([\n",
    "        pl.col('artist_id').apply(lambda x: x[:-3]).alias('train_item_ids'),\n",
    "        pl.col('artist_id').apply(lambda x: x[-3:]).alias('test_item_ids'),\n",
    "    ])\n",
    ")\n",
    "\n",
    "grouped_df_with_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dea774",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_seq_len = int(grouped_df_with_inds['train_item_ids'].apply(len).median())\n",
    "print(f\"средняя длина сессии {median_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# соберем строчки для разреженной матрицы\n",
    "rows = []\n",
    "cols = []\n",
    "values = []\n",
    "for user_id, train_ids, _ in grouped_df_with_inds.rows():\n",
    "    rows.extend([user_id] * len(train_ids))\n",
    "    values.extend([1] * len(train_ids))\n",
    "    cols.extend(train_ids)\n",
    "\n",
    "user_item_data = sp.csr_matrix((values, (rows, cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064a3aa",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "As a simple baseline, we will recommend the most popular artists.\n",
    "\n",
    "We want to first validate such a solution, which means we will consider only those artists who appear most frequently in `train_item_ids` as popular artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65a122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_artists = (\n",
    "    grouped_df_with_inds\n",
    "    .select(pl.col('train_item_ids').alias('artist_id'))\n",
    "    .explode('artist_id')\n",
    "    .groupby('artist_id')\n",
    "    .count()\n",
    "    .sort('count', descending=True)\n",
    "    .head(TOP_K + median_seq_len)\n",
    ")['artist_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_list = []\n",
    "hitrate_list = []\n",
    "\n",
    "for user_id, user_history, y_rel in grouped_df_with_inds.rows():\n",
    "    y_rec = top_artists.copy()\n",
    "    \n",
    "    ndcg_list.append(user_ndcg(y_rel, y_rec))\n",
    "    hitrate_list.append(user_hitrate(y_rel, y_rec))\n",
    "    \n",
    "print(f'NDCG@{TOP_K} = {np.mean(ndcg_list):.5f}, Hitrate@{TOP_K} = {np.mean(hitrate_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d3687",
   "metadata": {},
   "source": [
    "Don't forget about filtering out what has already been viewed (for different domains and approaches, this doesn't always improve recommendations, but in this case, it provided a boost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_list = []\n",
    "hitrate_list = []\n",
    "\n",
    "for user_id, user_history, y_rel in grouped_df_with_inds.rows():\n",
    "    y_rec = [artist_id for artist_id in top_artists if artist_id not in user_history]\n",
    "    \n",
    "    ndcg_list.append(user_ndcg(y_rel, y_rec))\n",
    "    hitrate_list.append(user_hitrate(y_rel, y_rec))\n",
    "    \n",
    "print(f'NDCG@{TOP_K} = {np.mean(ndcg_list):.5f}, Hitrate@{TOP_K} = {np.mean(hitrate_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7373ba7",
   "metadata": {},
   "source": [
    "## Building a Recommendations file\n",
    "\n",
    "To build recommendations, we can now consider all possible data. It is important to note that previously, to optimize memory, ids were converted to an integer format. However, for the production display, it is necessary to convert them back to the original identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32275e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_artists = (\n",
    "    data\n",
    "    .groupby('artist_id')\n",
    "    .count()\n",
    "    .sort('count', descending=True)\n",
    "    .head(TOP_K + median_seq_len)\n",
    ")['artist_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cb241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = []\n",
    "\n",
    "for user_id, user_history in data.groupby('user_id').agg(pl.col('artist_id')).rows():\n",
    "    y_rec = top_artists.copy()\n",
    "    \n",
    "    submission.append((user_id, y_rec))\n",
    "    \n",
    "submission = pl.DataFrame(submission, schema=('user_id', 'y_rec'))\n",
    "submission.write_parquet('sample_submission.parquet')\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f7ac3",
   "metadata": {},
   "source": [
    "! It's important to remember that the recommendations file should contain the original identifiers (strings), not those converted to numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa830df",
   "metadata": {},
   "source": [
    "### W2V in RecSys using Gensim library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4aad4a",
   "metadata": {},
   "source": [
    "To apply the W2V (Word2Vec) algorithm without hyperparameter tuning, we have the W2V algorithm and an array of sessions that are used as training data.\n",
    "\n",
    "Launch this algorithm on this dataset in a single line.\n",
    "\n",
    "To validate this model, for each training session, call the method predict output word using the standard model.\n",
    "\n",
    "If our model returns nothing (either an exception word or it has not been used before), we'll make the hitrate equal to zero and skip this example.\n",
    "\n",
    "If we have recommendations, using them, we'll filter out those objects that were already in the training sample, and then assess their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    ndcg_list = []\n",
    "    hitrate_list = []\n",
    "    for train_ids, y_rel in grouped_df_with_inds.select('train_item_ids', 'test_item_ids').rows():\n",
    "        model_preds = model.predict_output_word(\n",
    "            train_ids, topn=(TOP_K + len(train_ids))\n",
    "        )\n",
    "        if model_preds is None:\n",
    "            hitrate_list.append(0)\n",
    "            continue\n",
    "\n",
    "        y_rec = [pred[0] for pred in model_preds if pred[0] not in train_ids]\n",
    "        ndcg_list.append(user_ndcg(y_rel, y_rec))\n",
    "        hitrate_list.append(user_hitrate(y_rel, y_rec))\n",
    "    return np.mean(ndcg_list), np.mean(hitrate_list)\n",
    "\n",
    "# обучим w2v с параметрами по умолчанию\n",
    "model = Word2Vec(grouped_df_with_inds['train_item_ids'].to_list())\n",
    "mean_ndcg, mean_hitrate = evaluate_model(model)\n",
    "print(f'NDCG@{TOP_K} = {mean_ndcg:.5f}, Hitrate@{TOP_K} = {mean_hitrate:.5f}')\n",
    "\n",
    "#MAP@10 = 0.0033 Hitrate@10 = 0.1210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5712d",
   "metadata": {},
   "source": [
    "To find the optimal hyperparameters, we're starting from a baseline where NDCG was 0.0174.\n",
    "\n",
    "To adjust the parameters, we'll use Optuna.\n",
    "\n",
    "We have a set of hyperparameters to consider:\n",
    "\n",
    "- SKIP-GRAM algorithm (whether to use it or not)\n",
    "- The window parameter (the length of the window used for training)\n",
    "- The ns_exponent and negative parameters\n",
    "- The min_count parameter (filters objects that appear less than a certain number of times)\n",
    "- The vector_size parameter (determines the dimensionality of the embedding space; the larger it is, the more parameters can be trained, but this does not mean that the final model will be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def objective(trial):\n",
    "    sg = trial.suggest_categorical('sg', [0, 1])\n",
    "    window = trial.suggest_int('window', 1, 10)\n",
    "    ns_exponent = trial.suggest_float('ns_exponent', -3, 3)\n",
    "    negative = trial.suggest_int('negative', 8, 20)\n",
    "    min_count = trial.suggest_int('min_count', 3, 20)\n",
    "    vector_size = trial.suggest_categorical('vector_size', [64, 128])\n",
    "    \n",
    "    print({\n",
    "        'sg': sg,\n",
    "        'window_len': window,\n",
    "        'ns_exponent': ns_exponent,\n",
    "        'negative': negative,\n",
    "        'min_count': min_count,\n",
    "        'vector_size': vector_size,\n",
    "    })\n",
    "\n",
    "    set_seed(SEED)\n",
    "    model = Word2Vec(\n",
    "        grouped_df_with_inds['train_item_ids'].to_list() + grouped_df_with_inds['test_item_ids'].to_list(),\n",
    "        window=window,\n",
    "        sg=sg,\n",
    "        hs=0,\n",
    "        min_count=min_count,\n",
    "        vector_size=vector_size,\n",
    "        negative=negative,\n",
    "        ns_exponent=ns_exponent,\n",
    "        seed=SEED,\n",
    "        epochs=50,\n",
    "    )\n",
    "    \n",
    "    mean_ndcg, mean_hitrate = evaluate_model(model)\n",
    "\n",
    "    print(f'NDCG@{TOP_K} = {mean_ndcg:.5f}, Hitrate@{TOP_K} = {mean_hitrate:.5f}')\n",
    "    \n",
    "    return mean_ndcg\n",
    "    \n",
    "    \n",
    "study = optuna.create_study(directions=('maximize',))\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aeaaa7",
   "metadata": {},
   "source": [
    "Выведем гиперпараметры лучшей версии модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660d322",
   "metadata": {},
   "source": [
    "Trial 368 finished with value: 0.06800619114110919 and parameters: {'sg': 0, 'window': 10, 'ns_exponent': 0.07411320142850895, 'negative': 17, 'min_count': 11, 'vector_size': 128}. Best is trial 368 with value: 0.06800619114110919.\n",
    "NDCG@20 = 0.06801, Hitrate@20 = 0.37866\n",
    "{'sg': 0, 'window_len': 10, 'ns_exponent': 0.21706766856178566, 'negative': 20, 'min_count': 11, 'vector_size': 128}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3f79e",
   "metadata": {},
   "source": [
    "Преейдем от integer идентификаторов к исходным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "model = Word2Vec(\n",
    "    grouped_df_with_inds['train_item_ids'].to_list(),\n",
    "    **study.best_params,\n",
    "    hs=0,\n",
    "    seed=SEED,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "#ndcg_list = []\n",
    "submission_check = []\n",
    "for user_id, train_item_ids, test_item_ids  in grouped_df_with_inds.select('user_id', 'train_item_ids', 'test_item_ids').rows():\n",
    "    combined_known_items = set(train_item_ids + test_item_ids)\n",
    "    model_preds = model.predict_output_word(combined_known_items, topn=(TOP_K + len(combined_known_items)))\n",
    "    if model_preds is None:\n",
    "        ndcg_list.append(0)\n",
    "        continue\n",
    "        \n",
    "    y_rec = [pred[0] for pred in model_preds if pred[0] not in combined_known_items]\n",
    "    \n",
    "    #ndcg_list.append(user_ndcg(combined_known_items, y_rec))\n",
    "\n",
    "    mapped_user_id = user_mapping_inverse[user_id]\n",
    "\n",
    "    mapped_y_rec = [artist_mapping_inverse[artist_id] for artist_id in y_rec] # Adjust based on your actual logic\n",
    "    \n",
    "    submission_check.append((mapped_user_id, mapped_y_rec))\n",
    "\n",
    "submission_check = pl.DataFrame(submission_check, schema=('user_id', 'y_rec'))\n",
    "submission_check\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289874eb",
   "metadata": {},
   "source": [
    "Save the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d41194",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_check.write_parquet('sample_submission.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd75b56",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "NDCG@20 = 0.08397145641325111\n",
    "\n",
    "Hitrate@20 = 0.36992"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark_t1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
